{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\czt3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\czt3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\czt3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\czt3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected invalid input:  ['']\n"
     ]
    }
   ],
   "source": [
    "# Read reviews from training file\n",
    "reviews = open('data/train.txt', 'r').read().split('\\n')\n",
    "reviews = [line.split('\\t') for line in reviews]\n",
    "\n",
    "# Validate reviews strings\n",
    "for r in reviews:\n",
    "    if len(r) is not 2:\n",
    "        print('Rejected invalid input: ', str(r))\n",
    "        reviews.remove(r)\n",
    "\n",
    "# Split positive and negative reviews\n",
    "rw_pos = [r[0] for r in reviews if r[1] == '1']\n",
    "rw_neg = [r[0] for r in reviews if r[1] == '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# Select word types\n",
    "'''\n",
    "    J -> adjective\n",
    "    R -> adverb\n",
    "    V -> verb\n",
    "    N -> noun\n",
    "'''\n",
    "sel_wts = ['J', 'V', 'R', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Extract keywords\n",
    "def parse_review(r):\n",
    "    bow = []\n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', r)\n",
    "    # Tokenise\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    # Remove stop words\n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    # Part of Speed labelling \n",
    "    labelled = nltk.pos_tag(stopped)\n",
    "    # Reserve word types needed\n",
    "    for w in labelled:\n",
    "        w_type = w[1][0]\n",
    "        if w_type in sel_wts:\n",
    "            word = w[0].lower()\n",
    "            if w_type == 'V': # convert verbs to present tense\n",
    "                word = lemmatizer.lemmatize(word, 'v')\n",
    "            if w_type == 'N': # convert noun to single form\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            bow.append(word)\n",
    "    return bow\n",
    "    \n",
    "# Extract all selected words from parsed reviews\n",
    "all_words = []\n",
    "for r in rw_pos+rw_neg:\n",
    "    all_words = all_words + parse_review(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words frequency\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "# Reject low frequency words\n",
    "all_words = [word for word, freq in all_words.items() if freq > 1]\n",
    "# all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse reviews to vectors\n",
    "null_count = 0\n",
    "def rw_to_vec(r):\n",
    "    bow = parse_review(r)\n",
    "    if len(bow) == 0:\n",
    "        global null_count\n",
    "        null_count += 1\n",
    "    vec = {}\n",
    "    for w in all_words:\n",
    "        vec[w] = (w in bow)\n",
    "    return vec\n",
    "    \n",
    "rw_pos_vecs = [(rw_to_vec(r), 1) for r in rw_pos]\n",
    "rw_neg_vecs = [(rw_to_vec(r), 0) for r in rw_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter review vectors that has no word matched\n",
    "rw_vecs = []\n",
    "for tp in rw_pos_vecs + rw_neg_vecs:\n",
    "    vec = tp[0]\n",
    "    wd_matched = [k for k,v in vec.items() if v]\n",
    "    if len(wd_matched) > 0:\n",
    "        rw_vecs.append(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the vectors\n",
    "random.shuffle(rw_vecs)\n",
    "\n",
    "# Split train & test set\n",
    "train_set_pct = 0.8\n",
    "split_num = math.floor(len(rw_vecs) * train_set_pct)\n",
    "train_set = rw_vecs[:split_num]\n",
    "test_set = rw_vecs[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (classifier)\n",
    "model = nltk.NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 82.19895287958116\n",
      "Most Informative Features\n",
      "                   worst = True                0 : 1      =      8.5 : 1.0\n",
      "                friendly = True                1 : 0      =      7.2 : 1.0\n",
      "                    wait = True                0 : 1      =      6.7 : 1.0\n",
      "                    love = True                1 : 0      =      6.3 : 1.0\n",
      "                    take = True                0 : 1      =      5.5 : 1.0\n",
      "                    dont = True                0 : 1      =      5.1 : 1.0\n",
      "                    much = True                0 : 1      =      5.1 : 1.0\n",
      "                    hour = True                0 : 1      =      5.1 : 1.0\n",
      "                   tasty = True                1 : 0      =      4.9 : 1.0\n",
      "                    menu = True                1 : 0      =      4.9 : 1.0\n",
      "                   clean = True                1 : 0      =      4.2 : 1.0\n",
      "                    beer = True                1 : 0      =      4.2 : 1.0\n",
      "                     not = True                0 : 1      =      3.9 : 1.0\n",
      "                     pay = True                0 : 1      =      3.7 : 1.0\n",
      "                    felt = True                0 : 1      =      3.7 : 1.0\n",
      "                    live = True                0 : 1      =      3.7 : 1.0\n",
      "                  always = True                1 : 0      =      3.7 : 1.0\n",
      "                  tender = True                1 : 0      =      3.6 : 1.0\n",
      "                      so = True                1 : 0      =      3.6 : 1.0\n",
      "                 portion = True                1 : 0      =      3.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show accuracy and most informative features\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(model, test_set))*100)\n",
    "model.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise model and save as file\n",
    "model_file = open('naive_bayes.classifier', 'wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model from file\n",
    "model_file = open('naive_bayes.classifier', 'rb')\n",
    "model = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classification\n",
    "# def sentiment_classify(r):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9931738955798286\n",
      "0 0.006826104420170728\n"
     ]
    }
   ],
   "source": [
    "prob_dist = model.prob_classify(rw_to_vec(\n",
    "\"The flair bartenders are absolutely amazing!\"\n",
    "))\n",
    "for k in prob_dist.samples():\n",
    "    print(k, prob_dist.prob(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(test_set[50][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
